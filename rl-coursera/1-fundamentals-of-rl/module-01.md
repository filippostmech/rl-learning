# Module 01: The K-Armed Bandit Problem
- Reading: Chapter 2-2.7 (Pages 25-36)

- Lesson 1: The K-Armed Bandit Problem
    - Define reward
    - Understand the temporal nature of the bandit problem
    - Define k-armed bandit
    - Define action-values
<hr>    
- Lesson 2: What to Learn? Estimating Action Values
    - Define action-value estimation methods
    - Define exploration and exploitation
    - Select actions greedily using an action-value function
    - Define online learning
    - Understand a simple online sample-average action-value estimation method
    - Define the general online update equation
    - Understand why we might use a constant stepsize in the case of non-stationarity
<hr>
- Lesson 3: Exploration vs. Exploitation Tradeoff
    - Define epsilon-greedy
    - Compare the short-term benefits of exploitation and the long-term benefits of exploration
    - Understand optimistic initial values
    - Describe the benefits of optimistic initial values for early exploration
    - Explain the criticisms of optimistic initial values
    - Describe the upper confidence bound action selection method
    - Define optimism in the face of uncertainty